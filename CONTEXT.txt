STORYWEAVER AI - PROJECT CONTEXT
=================================

PRODUCT OVERVIEW
----------------
StoryWeaver AI (AI Ghost Writer) transforms personal anecdotes into professionally written books through AI-powered interviewing, refinement, and compilation.

Core Value: Democratize book authorship by replacing expensive ghostwriters with intelligent AI that captures memories, asks follow-up questions, refines narratives, and weaves them into cohesive manuscripts.


TECH STACK
----------
Frontend: Next.js 14 (App Router), TypeScript, Tailwind CSS, Zustand, React Query
Backend: Supabase (PostgreSQL, Auth, Storage, Edge Functions)
AI: Anthropic Claude API (claude-sonnet-4-20250514)
Vector DB: Supabase pgvector (for RAG)
Hosting: Vercel + Supabase


CORE ENTITIES
-------------
User -> Projects -> Anecdotes -> Media
                        |
                  AI Processing
                        |
              Book Compilations -> Versions


USER FLOW
---------

1. Project Creation
   User creates project with: title, description, genre, audience, tone, AI settings (question rounds)

2. Anecdote Writing
   User writes memory with: content, time frame, location, people, themes, media attachments

3. AI Processing Pipeline
   Draft -> Lock -> Question Generation -> Q&A Rounds -> Refinement -> Summarization -> Complete

   Stage Details:
   - Lock: Submit for AI analysis
   - Questions: AI generates 3-5 follow-up questions per round
   - Q&A: User answers; repeats for configured rounds (1-5)
   - Refine: AI compiles all into polished narrative
   - Summarize: AI creates 500-word and 100-word summaries

4. Book Compilation
   - Select completed anecdotes
   - AI generates chapter structure
   - AI writes transitions between anecdotes
   - AI generates introduction/conclusion
   - Version control for revisions
   - Export to DOCX/PDF


DATABASE SCHEMA
---------------

users
  id, email, display_name, credits_balance, subscription_tier

projects
  id, user_id, title, description, genre, target_audience, tone_style, question_rounds, status

anecdotes
  id, project_id, user_id, raw_content, status, questionnaire (JSONB), ai_refined_content, ai_summary_500, ai_summary_100, time_frame_start/end, people_involved, themes
  Status values: draft, locked, questioning, refining, completed

media_attachments
  id, anecdote_id, media_type, file_url, transcription, ai_description

book_compilations
  id, project_id, version_number, manuscript_content, chapter_structure (JSONB), included_anecdote_ids, docx_url, pdf_url

anecdote_chunks (RAG)
  id, anecdote_id, project_id, chunk_text, chunk_type, embedding (vector), metadata


API STRUCTURE
-------------

Auth
  /auth/signup
  /auth/signin
  /auth/signout

Projects
  GET/POST /api/projects
  GET/PATCH/DELETE /api/projects/:id

Anecdotes
  GET/POST /api/projects/:id/anecdotes
  GET/PATCH/DELETE /api/anecdotes/:id
  POST /api/anecdotes/:id/lock - Start AI processing
  POST /api/anecdotes/:id/answer - Submit Q&A answers
  POST /api/anecdotes/:id/complete - Finalize

Compilation
  POST /api/projects/:id/compile - Start compilation
  GET /api/compilations/:id - Get result
  POST /api/compilations/:id/export - Export to file

RAG
  POST /api/rag/ingest - Index anecdote
  POST /api/rag/search - Semantic search


AI PROCESSING DETAILS
---------------------

Question Generation
  Input: raw content, images, metadata
  Output: 3-5 targeted questions (sensory details, emotions, context)
  Credits: 2 per round

Content Refinement
  Input: raw content + all Q&A
  Output: polished 500-2000 word narrative
  Credits: 5

Summarization
  Output: 500-word (full context) + 100-word (quick reference)
  Credits: 2

Book Compilation
  Uses RAG to retrieve relevant anecdotes
  Generates chapter structure, transitions, intro/conclusion
  Credits: 20


RAG SYSTEM
----------

Purpose: Handle large projects exceeding context window limits

Flow:
  1. Ingest: Chunk anecdotes -> Generate embeddings -> Store in pgvector
  2. Retrieve: Embed query -> Similarity search -> Filter by metadata
  3. Generate: Construct prompt with retrieved context -> Claude generates

Chunk Types: raw, refined, summary_500, summary_100, qa

Use Cases:
  - Chapter generation (retrieve relevant anecdotes)
  - Transition writing (retrieve adjacent content)
  - Consistency checking (retrieve entity mentions)
  - Theme extraction (cluster by similarity)


CREDIT SYSTEM
-------------

Operation                 Credits
---------------------------------
Question generation       2/round
Refinement               5
Summarization            2
Image analysis           1
Audio transcription      3/min
Book compilation         20

Free tier: 100 credits on signup


FILE STRUCTURE
--------------

apps/web/
  app/
    (auth)/login, callback
    (dashboard)/projects/[id]/anecdotes/[id]
    api/projects, anecdotes, compile, rag
  components/ui, features, shared
  hooks/use-project, use-anecdote
  lib/supabase, ai, utils
  stores/
  types/

supabase/
  functions/process-anecdote, compile-book
  migrations/

services/api/ (if FastAPI/Express)


KEY FEATURES
------------

1. Google OAuth authentication
2. Projects with configurable AI settings
3. Anecdotes with rich text, media, metadata
4. Multi-round AI questioning
5. AI narrative refinement
6. Automatic summarization
7. RAG-powered book compilation
8. Version control for manuscripts
9. Export to DOCX/PDF
10. Credit-based billing


DEVELOPMENT COMMANDS
--------------------

npm run dev          - Start development
npm run build        - Production build
npm run lint         - Lint code
npm run test         - Run tests
npx supabase start   - Local Supabase
npx supabase db push - Apply migrations


ENVIRONMENT VARIABLES
---------------------

NEXT_PUBLIC_SUPABASE_URL
NEXT_PUBLIC_SUPABASE_ANON_KEY
SUPABASE_SERVICE_ROLE_KEY
ANTHROPIC_API_KEY
OPENAI_API_KEY (for embeddings)


NAMING CONVENTIONS
------------------

Files: PascalCase components, camelCase utils, kebab-case folders
Variables: camelCase
Types/Interfaces: PascalCase
Database: snake_case tables/columns
API: REST, plural nouns, proper HTTP methods


ANECDOTE STATUS FLOW
--------------------

draft -> locked -> questioning -> refining -> completed
            (user clicks Lock)  (AI asks)   (AI refines) (user approves)

User can unlock completed anecdote to edit, then re-lock for reprocessing.


BOOK COMPILATION FLOW
---------------------

1. User selects completed anecdotes
2. System retrieves all summaries via RAG
3. AI generates chapter outline (groups by theme/time)
4. For each chapter:
   - Retrieve full anecdote content
   - Retrieve supporting context
   - Generate chapter with transitions
5. Generate introduction and conclusion
6. Assemble manuscript
7. Save as new version
8. User reviews and provides feedback
9. Regenerate sections as needed
10. Export final manuscript


MEDIA SUPPORT
-------------

Images: JPEG, PNG, GIF, WebP (max 10MB)
Audio: MP3, WAV, M4A (max 100MB, 30min)
Video: MP4, MOV, WebM (max 500MB, 10min)
Documents: PDF, DOCX, TXT

AI analyzes images for context
AI transcribes audio/video
All media stored in Supabase Storage


PROJECT SETTINGS
----------------

- Title and description
- Genre (memoir, biography, self-help, etc.)
- Target audience (children, young adult, adult)
- Tone (conversational, formal, humorous, dramatic)
- Question rounds (1-5, default 3)
- Cover image


CURRENT STATUS
--------------

MVP Phase - Building core anecdote lifecycle and AI processing pipeline.

Priority Order:
1. Database schema + Auth
2. Projects/Anecdotes CRUD
3. AI question generation
4. AI refinement
5. Book compilation with RAG
6. Export functionality


BUILD ORDER
-----------

Backend first approach:
- Week 1: Schema + Auth + Project CRUD
- Week 2: Anecdote CRUD + AI endpoints
- Week 3: Frontend scaffold + Projects UI
- Week 4: Anecdote editor + AI integration
- Week 5-6: Book compilation + RAG system


KEY TECHNICAL DECISIONS
-----------------------

- Supabase over custom backend (speed, cost, auth included)
- pgvector over Pinecone (already using Supabase)
- Edge Functions for AI processing (serverless, scalable)
- OpenAI embeddings for RAG (quality, cost balance)
- Paragraph-based chunking with 500 token max
- Credit system for sustainable AI costs


TYPICAL ANECDOTE SIZE
---------------------

Raw content: 500-1000 words
Q&A total: 300-600 words
Refined content: 800-1500 words
500-word summary: 500 words
100-word summary: 100 words

Total per anecdote: ~2500 words / ~3000 tokens


CONTEXT WINDOW MANAGEMENT
-------------------------

Claude context: ~200K tokens
Typical book: 40 anecdotes x 3000 tokens = 120K tokens

RAG retrieves only relevant chunks:
- Chapter generation: ~8K tokens context
- Transition writing: ~2K tokens context
- Consistency check: ~4K tokens context


ERROR HANDLING
--------------

- AI failures: Retry with backoff, refund credits
- Upload failures: Resume capability
- Compilation failures: Save progress, allow resume
- All errors logged with context


SECURITY
--------

- Google OAuth only (no password storage)
- Row Level Security on all tables
- JWT with short expiry + refresh rotation
- All API inputs validated with Zod
- Media files scanned for safety
- HTTPS everywhere


END OF CONTEXT